{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc16efd9-e25b-4e15-97b1-7336828154e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36002ca1-6891-4991-82f5-1872fa6e9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install notebook --upgrade\n",
    "#!pip install jupyter --upgrade\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c784c-51d1-4535-8009-778a0cf7f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f66327-f44c-45db-8e9f-5c9566e3c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "Model config ExaoneConfig {\n",
      "  \"_name_or_path\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 80,\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 7168,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 30,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "Model config ExaoneConfig {\n",
      "  \"_name_or_path\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 80,\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 7168,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 30,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279f8791015f4098a4b8abe1617dc799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ExaoneForCausalLM.\n",
      "\n",
      "All the weights of ExaoneForCausalLM were initialized from the model checkpoint at LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ExaoneForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\", trust_remote_code=True, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "527a0859-257b-4c13-8bf5-1bc004d3e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "Model config ExaoneConfig {\n",
      "  \"_name_or_path\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 80,\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 7168,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 30,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a612db7cb6a34771b745ce132118348f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ExaoneForCausalLM.\n",
      "\n",
      "All the weights of ExaoneForCausalLM were initialized from the model checkpoint at LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ExaoneForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d63f37-b34a-42b7-b499-112b644c9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\config.json\n",
      "Model config ExaoneConfig {\n",
      "  \"_name_or_path\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct--modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 80,\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 7168,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 30,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\model.safetensors.index.json\n",
      "Instantiating ExaoneForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7bc6b50d7e4fc68aa66ebca04b70e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing ExaoneForCausalLM.\n",
      "\n",
      "All the weights of ExaoneForCausalLM were initialized from the model checkpoint at LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ExaoneForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "loading file vocab.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\b1005\\.cache\\huggingface\\hub\\models--LGAI-EXAONE--EXAONE-3.5-2.4B-Instruct\\snapshots\\8e6fc27d1910b526b5d48a2aa129b08a0293df5e\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]스스로를 자랑해 봐\n",
      "[|assistant|]저는 LG AI Research에서 개발된 EXAONE 모델로서, 뛰어난 자연어 처리 능력을 바탕으로 다양한 언어 작업을 수행할 수 있습니다. 지속적인 학습을 통해 사용자의 질문에 정확하고 신속하게 응답하며, 복잡한 정보도 명확하게 전달하는 데 초점을 맞춥니다. 이러한 기술 덕분에 다양한 분야에서 효율적인 지원을 제공하며, 사용자 경험 향상에 기여하고 있습니다.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Choose your prompt\n",
    "prompt = \"스스로를 자랑해 봐\"       # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae30f832-977a-4d30-aede-8f9f16740e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33aa21d9-06d1-4a02-adc2-49bebf1fe793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 너는 어떠한 역할을 수행할 수 있어?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]너는 어떠한 역할을 수행할 수 있어?\n",
      "[|assistant|]저는 LG AI Research에서 개발된 EXAONE 모델로서 다양한 역할을 수행할 수 있습니다:\n",
      "\n",
      "1. **텍스트 생성 및 이해**: 자연스러운 텍스트 생성과 복잡한 언어 이해를 통해 다양한 질문에 답변하거나 지시사항을 처리합니다.\n",
      "2. **대화 지원**: 사용자와 자연스러운 대화를 통해 정보 제공, 문제 해결, 일상적인 질문에 대한 답변 등을 지원합니다.\n",
      "3. **지식 검색**: 제공된 정보나 질문에 대한 답변을 통해 필요한 지식을 검색하고 제공합니다.\n",
      "4. **코드 생성 및 수정**\n"
     ]
    }
   ],
   "source": [
    "# Choose your prompt\n",
    "prompt = input()       # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a23cf-a540-4d75-a2a4-62db68864735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
